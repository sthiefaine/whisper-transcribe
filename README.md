# ğŸ¤ Whisper Service - La BoÃ®te de Chocolat

Service de transcription audio utilisant Whisper.cpp pour le podcast "La BoÃ®te de Chocolat".

## ğŸš€ DÃ©ploiement

### DÃ©marrage Rapide (RecommandÃ©)
```bash
# Cloner le projet
git clone https://github.com/votre-username/laboitedechocolat-whisper.git
cd laboitedechocolat-whisper

# DÃ©marrage automatique
npm start

# Test rapide
npm run test-quick
```

### Avec Docker Compose (Local)
```bash
# DÃ©marrer les services
npm run up

# VÃ©rifier les logs
npm run logs
```

### Avec Railway (RecommandÃ©)
1. Connectez votre repo GitHub Ã  Railway
2. Railway dÃ©tectera automatiquement le Dockerfile
3. DÃ©ployez !

### Avec Render
1. CrÃ©ez un nouveau Web Service
2. Connectez votre repo GitHub
3. SÃ©lectionnez "Docker" comme runtime
4. DÃ©ployez !

## ğŸ“¡ API

### Transcription
```bash
POST /transcribe
Content-Type: application/json

{
  "audio_url": "https://example.com/audio.mp3",
  "language": "fr",
  "model": "base"
}
```

**RÃ©ponse :**
```json
{
  "success": true,
  "transcription": "Voici le texte transcrit...",
  "model": "whisper-base-fr",
  "processing_time": 12.34,
  "file_size": 1024000
}
```

### Health Check
```bash
GET /health
```

**RÃ©ponse :**
```json
{
  "status": "healthy",
  "timestamp": "2024-01-01T12:00:00",
  "model": "whisper-base-fr",
  "version": "1.0.0"
}
```

### Lister les ModÃ¨les
```bash
GET /models
```

**RÃ©ponse :**
```json
{
  "models": ["ggml-base.bin"],
  "current_model": "ggml-base.bin"
}
```

## ğŸ“¤ Gestion des longues transcriptions

Pour les fichiers audio gÃ©nÃ©rant une transcription longue (plus de 2000 caractÃ¨res), lâ€™API **ne renvoie pas tout le texte dans la rÃ©ponse**. Ã€ la placeâ€¯:

- La transcription est stockÃ©e cÃ´tÃ© serveur dans `/var/log/whisper/`.
- Lâ€™API renvoie une clÃ© `transcription_url` permettant de tÃ©lÃ©charger le texte.
- Pour les transcriptions courtes, le texte est renvoyÃ© directement dans la rÃ©ponse.

### Exemple de rÃ©ponse pour une longue transcription
```json
{
  "success": true,
  "transcription_url": "https://api.ton-domaine.com/transcriptions/12345678-90ab-cdef-1234-567890abcdef.txt",
  "model": "whisper-large-fr",
  "processing_time": 123.45,
  "file_size": 12345678
}
```

### Pour rÃ©cupÃ©rer la transcription

Il suffit dâ€™accÃ©der Ã  lâ€™URL fournieâ€¯:
```
GET /transcriptions/<transcription_id>.txt
```

---

## ğŸ”€ DiffÃ©rence entre `/transcribe` et `/transcribe/file`

- **`/transcribe/file`** : upload direct dâ€™un fichier audio (multipart/form-data)
- **`/transcribe`** : envoi dâ€™une URL dâ€™audio Ã  tÃ©lÃ©charger (JSON)

Les deux routes appliquent la mÃªme logiqueâ€¯: stockage + URL pour les longues transcriptions, texte direct pour les courtes.

## ğŸ”§ Configuration

### Variables d'environnement
- `PORT` : Port du serveur (dÃ©faut: 8080)
- `MAX_FILE_SIZE` : Taille max des fichiers (dÃ©faut: 100MB)
- `WHISPER_PATH` : Chemin vers Whisper.cpp (dÃ©faut: /opt/whisper.cpp)

### ModÃ¨les Disponibles
- `base` : ModÃ¨le de base (recommandÃ©)
- `small` : ModÃ¨le plus petit, plus rapide
- `medium` : ModÃ¨le plus grand, plus prÃ©cis
- `large` : ModÃ¨le le plus prÃ©cis (nÃ©cessite plus de RAM)

## ğŸ“Š Monitoring

- Health check automatique toutes les 30s
- Logs structurÃ©s avec timestamps
- MÃ©triques de performance (temps de traitement)
- Gestion des erreurs et timeouts

## ğŸ› ï¸ DÃ©veloppement

### Scripts NPM
Le projet utilise npm pour centraliser tous les scripts :

```bash
# Voir tous les scripts disponibles
npm run help

# Commandes principales
npm start          # DÃ©marrage automatique
npm run test-quick # Test rapide
npm run logs       # Voir les logs
npm run reset      # Reset complet
```

### Fichier de Test
Le projet inclut un fichier de test audio `data/avatar_test_short.mp3` (145MB) pour tester la transcription :
```bash
# VÃ©rifier le fichier
ls -lh data/avatar_test_short.mp3

# Test rapide
npm run test-quick
```

### Structure du Projet
```
laboitedechocolat-whisper/
â”œâ”€â”€ Dockerfile              # Image Docker
â”œâ”€â”€ docker-compose.yml      # Orchestration
â”œâ”€â”€ server.py              # Serveur Flask
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â”œâ”€â”€ nginx.conf            # Configuration Nginx
â”œâ”€â”€ .gitignore           # Fichiers ignorÃ©s
â”œâ”€â”€ README.md            # Documentation
â””â”€â”€ scripts/             # Scripts utilitaires
    â”œâ”€â”€ install-whisper.sh
    â””â”€â”€ download-models.sh
```

### Tests Locaux

#### Test avec Docker
```bash
# DÃ©marrer les services
npm run up

# Test rapide
npm run test-quick

# Test complet
npm test

# VÃ©rifier la santÃ©
npm run test-health
```

#### Test sans Docker (dÃ©veloppement)
```bash
# Mode dÃ©veloppement
npm run dev

# Dans un autre terminal, tester
npm run test-local
```

#### Test manuel avec curl
```bash
# Test avec fichier local
curl -X POST http://localhost:8080/transcribe/file \
  -F "audio_file=@data/avatar_test_short.mp3" \
  -F "language=fr" \
  -F "model=base"

# Test avec URL
curl -X POST http://localhost:8080/transcribe \
  -H "Content-Type: application/json" \
  -d '{"audio_url": "https://example.com/test.mp3"}'
```

## ğŸ”’ SÃ©curitÃ©

- Utilisateur non-root dans le conteneur
- Validation des URLs d'entrÃ©e
- Limitation de la taille des fichiers
- Timeouts configurÃ©s
- CORS configurÃ© (Ã  adapter en production)

## ğŸ“ˆ Performance

- ModÃ¨le base : ~1GB RAM, ~10-30s pour 1min d'audio
- OptimisÃ© pour les podcasts franÃ§ais
- Support des formats audio courants (MP3, WAV, M4A)
- Traitement asynchrone possible

## ğŸ¤ IntÃ©gration

### Avec votre projet principal
```typescript
// Dans votre projet laboitedechocolat
const WHISPER_API_URL = process.env.WHISPER_API_URL || "http://localhost:8080";

const response = await fetch(`${WHISPER_API_URL}/transcribe`, {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    audio_url: audioFileUrl,
    language: 'fr',
    model: 'base'
  })
});
```

## ğŸ› DÃ©pannage

### ProblÃ¨mes Courants

1. **ModÃ¨le non trouvÃ©**
   ```bash
   # VÃ©rifier que le modÃ¨le est tÃ©lÃ©chargÃ©
   docker-compose exec whisper-api ls /opt/whisper.cpp/models/
   ```

2. **Timeout de transcription**
   - Augmenter les timeouts dans nginx.conf
   - VÃ©rifier la taille du fichier audio

3. **Erreur de compilation**
   ```bash
   # Reconstruire l'image
   docker-compose build --no-cache
   ```

## ğŸ“ Licence

MIT License - Libre d'utilisation pour le projet "La BoÃ®te de Chocolat" 

## ğŸ“¦ ModÃ¨les Disponibles
- `tiny` : ModÃ¨le ultra-rapide, peu prÃ©cis
- `base` : ModÃ¨le de base (recommandÃ© pour la plupart des usages)
- `small` : Plus prÃ©cis, un peu plus lent
- `medium` : TrÃ¨s prÃ©cis, plus lent
- `large` : Le plus prÃ©cis, trÃ¨s gourmand en RAM/CPU
- `large-v2` : Variante amÃ©liorÃ©e du modÃ¨le large

## ğŸ“¥ TÃ©lÃ©charger tous les modÃ¨les Whisper

Pour tÃ©lÃ©charger tous les modÃ¨les sur votre serveur (aprÃ¨s dÃ©ploiement) :

```bash
bash scripts/download-all-models.sh
```

Les modÃ¨les seront placÃ©s dans `/opt/whisper.cpp/models` (ou le volume Docker correspondant).

## ğŸ“¡ Exemple d'appel API avec un modÃ¨le spÃ©cifique

### Transcription avec le modÃ¨le large

```bash
curl -X POST http://localhost:8080/transcribe/file \
  -F "audio_file=@data/avatar_test_short.mp3" \
  -F "language=fr" \
  -F "model=large"
```

## ğŸ³ Astuce Docker/Coolify

- Les modÃ¨les tÃ©lÃ©chargÃ©s sont persistants si vous utilisez un volume Docker pour `/opt/whisper.cpp/models`.
- AprÃ¨s chaque dÃ©ploiement, vÃ©rifiez la prÃ©sence des modÃ¨les avec :
  ```bash
  docker-compose exec whisper-api ls /opt/whisper.cpp/models/
  ``` 